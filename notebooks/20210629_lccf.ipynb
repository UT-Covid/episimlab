{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "538144c0-6592-447b-9f7e-e3eee6faae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import attr\n",
    "import math\n",
    "from pprint import pprint as pp\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e826779c-d0c1-499d-82b6-17a3f37db1c9",
   "metadata": {},
   "source": [
    "# 20210629_lccf\n",
    "\n",
    "----\n",
    "\n",
    "The purpose of this notebook is to generate test data for LCCF performance scaling experiments on the model run in the script `scripts/20210623_lccf.py`.\n",
    "\n",
    "In this notebook, I will load files `data/lccf/*.csv` and cp the rows to emulate large-scale Safegraph data ingests. The starting point of this notebook looks like:\n",
    "\n",
    "```bash\n",
    "$ ls ../data/lccf\n",
    "census_pop1_rows1.csv   contacts_pop1_rows1.csv travel_pop1_rows1.csv\n",
    "```\n",
    "\n",
    "The goal is to make `*rows2.csv`, `*rows4.csv`, etc. files that have 2x, 4x, etc. as many rows as the `*rows1.csv` files.\n",
    "\n",
    "----\n",
    "\n",
    "To test performance scaling as a function of population size, I will also make `*_pop2_rows0.csv`, `*_pop4_rows0.csv`, etc. which are equivalent to `*rows0.csv`, but with 2x, 4x, etc. greater contact/travel/census population. `pop1` is implied if no `pop` identifier is included in the file name.\n",
    "\n",
    "## Work Plan\n",
    "\n",
    "- Input: how many ZCTAs in the simulation?\n",
    "- Determine number of census DF replicates we need to concat.\n",
    "- Concat m replicates of census DF, with unique ZCTA IDs\n",
    "    - `prefix` column confers uniqueness\n",
    "- Concat m replicates of travel DF, with unique ZCTA IDs\n",
    "    - `prefix` column confers uniqueness\n",
    "    - Remember that this assumes no travel between replicates (cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f1a6c5a-5f00-43f8-b7cf-3a90af3c11b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGE_GROUPS = ('<5', '5-17', '18-49', '50-64', '65+', )\n",
    "N_ROWS = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea6dea5-86aa-4469-b808-19cbbce78517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "census_pop1_rows1.csv   contacts_pop1_rows1.csv travel_pop1_rows1.csv\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/lccf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbcabf68-97aa-4c15-86fe-72c2370c69cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> ../data/lccf/census_pop1_rows1.csv <==\n",
      "\"\",\"GEOID\",\"NAME\",\"age_bin\",\"group_pop\"\n",
      "\"1\",\"75001\",\"ZCTA5 75001\",\"<5\",794\n",
      "\"2\",\"75001\",\"ZCTA5 75001\",\"18-49\",9420\n",
      "\"3\",\"75001\",\"ZCTA5 75001\",\"5-17\",1404\n",
      "\"4\",\"75001\",\"ZCTA5 75001\",\"50-64\",2259\n",
      "\"5\",\"75001\",\"ZCTA5 75001\",\"65+\",1115\n",
      "\"6\",\"75002\",\"ZCTA5 75002\",\"<5\",4227\n",
      "\"7\",\"75002\",\"ZCTA5 75002\",\"18-49\",29659\n",
      "\"8\",\"75002\",\"ZCTA5 75002\",\"5-17\",15710\n",
      "\"9\",\"75002\",\"ZCTA5 75002\",\"50-64\",14706\n",
      "\n",
      "==> ../data/lccf/contacts_pop1_rows1.csv <==\n",
      ",age1,age2,daily_per_capita_contacts\n",
      "0,<5,<5,2.160940833918119\n",
      "1,5-17,<5,0.5973413405271149\n",
      "2,18-49,<5,0.3822025191217617\n",
      "3,50-64,<5,0.3523966597811896\n",
      "4,65+,<5,0.18975609071541075\n",
      "5,<5,5-17,2.164117384279739\n",
      "6,5-17,5-17,8.146970087503425\n",
      "7,18-49,5-17,2.431391745980527\n",
      "8,50-64,5-17,1.885100325362032\n",
      "\n",
      "==> ../data/lccf/travel_pop1_rows1.csv <==\n",
      ",Unnamed: 0,source,destination,age,n,date,destination_type\n",
      "30555,30555,76511,76511,<5,35.05384615384615,2020-03-11,local\n",
      "30556,30556,76511,76511,18-49,472.2846153846154,2020-03-11,local\n",
      "30557,30557,76511,76511,5-17,150.01538461538462,2020-03-11,local\n",
      "30558,30558,76511,76511,50-64,165.46923076923076,2020-03-11,local\n",
      "30559,30559,76511,76511,65+,121.36923076923075,2020-03-11,local\n",
      "30560,30560,76511,76530,<5,3.576923076923077,2020-03-11,local\n",
      "30561,30561,76511,76530,18-49,48.19230769230769,2020-03-11,local\n",
      "30562,30562,76511,76530,5-17,15.307692307692308,2020-03-11,local\n",
      "30563,30563,76511,76530,50-64,16.884615384615387,2020-03-11,local\n"
     ]
    }
   ],
   "source": [
    "!head ../data/lccf/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a99cc6-e1d2-4a64-a10c-7abf42249564",
   "metadata": {},
   "source": [
    "Are ZCTAs from census a subset of travel? Vice versa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cad788b-3626-452d-9968-3b2354694b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "@attr.s\n",
    "class ScalingExp:\n",
    "    \"\"\"One scaling experiment.\"\"\"\n",
    "\n",
    "    ref_census_csv = attr.ib(type=str)\n",
    "    ref_travel_csv = attr.ib(type=str)\n",
    "    ref_contacts_csv = attr.ib(type=str)\n",
    "    census_usecols = attr.ib(type=list, default=[])\n",
    "    census_travel = attr.ib(type=list, default=[])\n",
    "    census_contacts = attr.ib(type=list, default=[])\n",
    "    pop_factor = attr.ib(type=int, default=1)\n",
    "    n_zcta = attr.ib(type=int, default=None)\n",
    "    \n",
    "    def get_m_replicates(self):\n",
    "        n_zcta = None\n",
    "    \n",
    "    def get_extended_travel(self) -> pd.DataFrame:\n",
    "        c = self.census\n",
    "        t = self.travel\n",
    "        \n",
    "        return t.merge(c, left_on='source', right_on='GEOID')\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def census(self, force_refresh=False) -> pd.DataFrame:\n",
    "        if hasattr(self, '_census') and not force_refresh:\n",
    "            return self._census\n",
    "        else:\n",
    "            return self.parse_census()\n",
    "\n",
    "    @property\n",
    "    def contacts(self, force_refresh=False) -> pd.DataFrame:\n",
    "        if hasattr(self, '_contacts') and not force_refresh:\n",
    "            return self._contacts\n",
    "        else:\n",
    "            return self.parse_contacts()\n",
    "\n",
    "    @property\n",
    "    def travel(self, force_refresh=False) -> pd.DataFrame:\n",
    "        if hasattr(self, '_travel') and not force_refresh:\n",
    "            return self._travel\n",
    "        else:\n",
    "            return self.parse_travel()\n",
    "\n",
    "    def parse_census(self) -> pd.DataFrame:\n",
    "        df = pd.read_csv(self.ref_census_csv, usecols=self.census_usecols)\n",
    "        assert not df.isna().any().any(), ('found null values in df', df.isna().any())\n",
    "        # df.rename(columns={'GEOID': 'vertex', 'age_bin': 'age_group'}, inplace=True)\n",
    "        # df.set_index(['vertex', 'age_group'], inplace=True)\n",
    "        # filter to zcta that we want to model in the simulation (vertex coords)\n",
    "        self._census = df\n",
    "        return df\n",
    "\n",
    "    def parse_contacts(self) -> pd.DataFrame:\n",
    "        self._contacts = pd.read_csv(self.ref_contacts_csv, usecols=self.contacts_usecols)\n",
    "        return self._contacts\n",
    "\n",
    "    def parse_travel(self) -> pd.DataFrame:\n",
    "        self._travel = pd.read_csv(self.ref_travel_csv, usecols=self.travel_usecols)\n",
    "        return self._travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e6be615-49f2-4bb0-aa25-99c9ea0870dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 3 required positional arguments: 'census_usecols', 'census_travel', and 'census_contacts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-474b13f4271a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m exp = ScalingExp(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mref_census_csv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../data/lccf/census_pop1_rows1.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mref_travel_csv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../data/lccf/travel_pop1_rows1.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mref_contacts_csv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../data/lccf/contacts_pop1_rows1.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 3 required positional arguments: 'census_usecols', 'census_travel', and 'census_contacts'"
     ]
    }
   ],
   "source": [
    "exp = ScalingExp(\n",
    "    ref_census_csv='../data/lccf/census_pop1_rows1.csv',\n",
    "    ref_travel_csv='../data/lccf/travel_pop1_rows1.csv',\n",
    "    ref_contacts_csv='../data/lccf/contacts_pop1_rows1.csv',\n",
    ")\n",
    "exp.get_extended_travel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9885628-a7b2-410e-81ae-51e10ba60611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882a3dc9-b9eb-4411-a80d-563bc2a4788e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8f4809-1328-4bdb-9826-fc65f9efea93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ada9c6-6177-4be0-9754-133ebdfb0e41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfb0812-ac89-4ca9-b654-2285d5adcd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas = {\n",
    "    'census': {\n",
    "        'value_col': None,\n",
    "        'usecols': (\"GEOID\", \"age_bin\", \"group_pop\"),\n",
    "        'glob': list(),\n",
    "        'keys': None\n",
    "    },\n",
    "    'contacts': {\n",
    "        'value_col': None,\n",
    "        'usecols': ('age1', 'age2', 'daily_per_capita_contacts'),\n",
    "        'glob': list(),\n",
    "        'keys': None\n",
    "    },\n",
    "    'travel': {\n",
    "        'value_col': None,\n",
    "        'usecols': ('source', 'destination', 'age', 'n', 'date', 'destination_type'),\n",
    "        'glob': list(),\n",
    "        'keys': None\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8384275-8826-4bd3-9c6e-8de0b41832de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scaled_csvs(dir_fp: str, scales: tuple = (2, 4, 8), verbose=2):\n",
    "    assert os.path.isdir(dir_fp)\n",
    "    for schema in schemas:\n",
    "        query = f\"{schema}*.csv\"\n",
    "        hits = glob(os.path.join(dir_fp, query))\n",
    "        assert len(hits) == 1, f\"found {len(hits)} hits for glob query {query}, expected 1\"\n",
    "        schemas[schema]['glob'] = hits\n",
    "    if verbose:\n",
    "        print(f'schemas:')\n",
    "        pp(schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73e63ac-af90-454e-b20c-638c5644626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_scaled_csvs('../data/lccf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b421179f-b039-4aac-90f0-f029c73f3565",
   "metadata": {},
   "source": [
    "**Question**: what should we use as the `value_col` for each schema? Also gives us a reminder of what the schema is. Let's `head`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679c44cd-1629-4877-950b-c01f478c2921",
   "metadata": {},
   "source": [
    "We're going to need more than just cp rows. Specifically:\n",
    "- Census data probably needs unique `primaryKey == [GEOID, age_bin]`\n",
    "- Travel data needs n^2 rows where n is the number of GEOIDs\n",
    "\n",
    "So what we really mean by \"scaling rows\" is that we're scaling by `n`, number of GEOIDs in the census CSV. Easy way to do this is just prepend non-zero integer to GEOID.\n",
    "\n",
    "----\n",
    "\n",
    "## Parsers for each schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3873072-a66a-4b44-b780-df2d5c2f9e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_census(csv_fp: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_fp, usecols=schemas['census']['usecols'])\n",
    "    assert not df.isna().any().any(), ('found null values in df', df.isna().any())\n",
    "    # df.rename(columns={'GEOID': 'vertex', 'age_bin': 'age_group'}, inplace=True)\n",
    "    # df.set_index(['vertex', 'age_group'], inplace=True)\n",
    "    # filter to zcta that we want to model in the simulation (vertex coords)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7dfc09-8de5-4e69-8782-889ab95c3c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_contacts(csv_fp: str) -> pd.DataFrame:\n",
    "    return pd.read_csv(csv_fp, usecols=schemas['contacts']['usecols'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefea24d-f91f-4501-81c7-4297297be7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_travel(csv_fp: str) -> pd.DataFrame:\n",
    "    return pd.read_csv(csv_fp, usecols=schemas['travel']['usecols'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4cf65b-3fdb-4006-a793-b7773bf97b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "for schema in schemas:\n",
    "    schemas[schema]['parser'] = globals()[f'parse_{schema}']\n",
    "pp(schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4030bb38-463b-41f9-ae52-bb6f2b56f142",
   "metadata": {},
   "source": [
    "## Extend census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05591802-8ead-45a9-8086-b5979372e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extended_census(census_df: pd.DataFrame, n: int = 0) -> pd.DataFrame:\n",
    "    \"\"\"Returns a pd.DataFrame with `n` unique GEOID-like integers.\n",
    "    Iterates over GEOIDs in `census_df`, prepending positive integer\n",
    "    to generate unique IDs.\n",
    "    \"\"\"\n",
    "    assert not 'prefix' in census_df.columns\n",
    "    n = n if n else len(census_df)\n",
    "    out_df = census_df\n",
    "    out_df['prefix'] = 0\n",
    "    i = 0\n",
    "    while len(out_df) < n:\n",
    "        census_df['prefix'] = i\n",
    "        out_df = pd.concat((out_df,) + (census_df,) * i)\n",
    "        i += 1\n",
    "    out_df['unique_GEOID'] = (out_df['prefix'].astype(str) + \n",
    "                              out_df['GEOID'].astype(str)).astype(int)\n",
    "    # del out_df['prefix']\n",
    "    assert out_df['unique_GEOID'].unique().all()\n",
    "    return out_df.iloc[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f1621-47e1-4ce8-a9bc-d2ff1ddc379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "census_df = parse_census('../data/lccf/census_pop1_rows1.csv')\n",
    "pp(len(census_df))\n",
    "extended_census = get_extended_census(census_df, n=N_ROWS)\n",
    "schemas['census']['keys'] = extended_census['GEOID']\n",
    "extended_census"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331d4f2b-1b7d-42e8-b180-f4e1a2109907",
   "metadata": {},
   "source": [
    "## Generate unique pairwise combinations of GEOIDs for travel data\n",
    "\n",
    "We assume the worst case scenario here: a length n^2 index for n GEOIDs. Multiply by any other demographic coordinates, in this case, age group. In reality, probably a few less, since the graph of travel between GEOIDs is not complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b1d513-26fe-4d94-a6e1-86dfd4cdd583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_pairs(geoids: pd.Series, age_groups: pd.Series) -> pd.DataFrame:\n",
    "    return (geoids\n",
    "            .to_frame(name='source')\n",
    "            # like itertools.product\n",
    "            .merge(geoids.to_frame(name='destination'), how='cross')\n",
    "            .merge(age_groups.to_frame(name='age'), how='cross')\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556150de-5d6c-4b31-aa13-b3ca5ca8542b",
   "metadata": {},
   "source": [
    "Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dc67de-3a30-416b-89b1-a6b580921ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c11cfc-223c-4e3d-a463-847fb1c9986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_travel.merge(census_df, left_on='source', right_on='unique_GEOID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86250315-4058-499f-a8c3-cb16c40f95e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ab65ea-1226-4e2d-b2ac-401f9199f75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_get_pairs = get_unique_pairs(schemas['census']['keys'], pd.Series(AGE_GROUPS))\n",
    "test_get_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba124b9-6659-4057-a97e-526f93e4e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_rows = (N_ROWS**2)*5\n",
    "print(f'We expect there to be {expected_rows} rows in the above frame')\n",
    "assert test_get_pairs.shape[0] == expected_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfb3456-45a8-4e53-b137-ba0038e926e2",
   "metadata": {},
   "source": [
    "## Fabricate Travel Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91fccd5-0f3c-4223-a9d5-4f3440053786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extended_travel(ref_travel: pd.DataFrame, census_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # get unique pairs\n",
    "    # construct DF\n",
    "    out = get_unique_pairs(census_df['unique_GEOID'], pd.Series(AGE_GROUPS))\n",
    "    out['n'] = 0\n",
    "    # set index for both DFs\n",
    "#     ref_travel = ref_travel.set_index(['source', 'destination'], inplace=False)\n",
    "#     census_df = (census_df\n",
    "#                  .rename(columns={'unique_GEOID': 'source'}, inplace=False)\n",
    "#                  .set_index(['source'], inplace=False))\n",
    "    \n",
    "    # merge on index\n",
    "    # fillna zero if ref_travel index DNE\n",
    "    return census_df\n",
    "    return (ref_travel\n",
    "            .merge(census_df, left_on='source', right_on='unique_GEOID')\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64e3261-1425-41c7-8bab-e856f33e280a",
   "metadata": {},
   "source": [
    "Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b47e84-7ca4-448f-884d-aae9990d203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_travel = parse_travel('../data/lccf/travel_pop1_rows1.csv')\n",
    "extended_travel = get_extended_travel(ref_travel, extended_census)\n",
    "extended_travel.unique_GEOID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1805072b-cfbb-433c-814b-658aa0d18777",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_travel.source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefde6b1-5e04-443b-b661-079018fd5a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_travel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd59d0c-face-471a-a123-477d318fee8c",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9304c780-f453-4b2a-b663-d01b9533debd",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas['census']['keys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49152be1-866a-456a-a042-69ac6ac682a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_census('../data/lccf/census_pop1_rows1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7955439f-44f7-4160-83f8-e8016a6607c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36ceece-7e04-43f2-a78d-83b1f82e23e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_travel('../data/lccf/travel_pop1_rows1.csv')['n'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c62b0c-855a-4add-a4d5-6292b356ec04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
